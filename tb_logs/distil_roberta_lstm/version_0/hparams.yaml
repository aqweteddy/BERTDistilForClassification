batch_size: 32
bert_model:
  ckpt: tb_logs/roberta/version_1/epoch=3.ckpt
  num_classes: 16
data:
  maxlen: 350
description: MSE(logits)
loss_a: 0.0
lr: 0.005
lstm_model:
  dropout: 0.3
  embed_size: 250
  hid_size: 256
  num_classes: 16
  num_layers: 3
  pretrained_weight: true
  with_attn: false
name: distil_roberta_lstm
weight_decay: 0
